# ğŸ¤– AI-Powered Customer Support Chatbot

A modern **AI-powered customer support chatbot** built from scratch using **Ollama** and **JavaScript**.
The chatbot supports **FAQ grounding**, **real-time AI responses**, and a **dark/light mode UI**, closely mimicking real-world customer support systems.

---

## âœ¨ Features

* ğŸ’¬ **Back-and-forth chat interface** (real chat bubbles)
* ğŸ§  **FAQ grounding** (answers from predefined business FAQs first)
* ğŸ¤– **AI fallback** using a locally hosted LLM via **Ollama**
* ğŸŒ— **Dark / Light mode** with persistence
* âš¡ **Real-time responses**
* ğŸ›¡ï¸ **Graceful error handling** (never hangs on â€œTypingâ€¦â€)
* ğŸ”’ **Local AI inference** (no paid APIs, privacy-friendly)

---

## ğŸ—ï¸ Tech Stack

| Layer    | Technology                   |
| -------- | ---------------------------- |
| Frontend | HTML, CSS, JavaScript        |
| Backend  | Node.js, Express             |
| AI Model | Ollama (Mistral)             |
| Styling  | Custom CSS (Dark/Light mode) |

---

## ğŸ“ Project Structure

```
ai-support-chatbot/
â”‚
â”œâ”€â”€ client/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ style.css
â”‚   â””â”€â”€ script.js
â”‚
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ index.js
â”‚   â”œâ”€â”€ faqs.js
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ README.md
```

---

## ğŸ§  How It Works

1. User sends a message from the chat UI
2. Backend checks if the query matches any **FAQs**
3. If found â†’ returns FAQ response (fast & deterministic)
4. If not â†’ forwards query to **Ollama LLM**
5. AI-generated response is sent back to the UI
6. UI updates with clean chat bubbles

---

## ğŸš€ Getting Started (Local Setup)

### âœ… Prerequisites

Make sure you have the following installed:

* **Node.js** (v18+ recommended)
* **Ollama** â†’ [https://ollama.com/download](https://ollama.com/download)

---

### ğŸ”¹ Step 1: Clone the Repository

```bash
git clone https://github.com/your-username/ai-support-chatbot.git
cd ai-support-chatbot
```

---

### ğŸ”¹ Step 2: Install Ollama Model

```bash
ollama pull mistral
```

Run once to verify:

```bash
ollama run mistral
```

---

### ğŸ”¹ Step 3: Setup Backend

```bash
cd server
npm install
npm install node-fetch
```

Start the server:

```bash
node index.js
```

You should see:

```
Server running on http://localhost:5000
```

---

### ğŸ”¹ Step 4: Run Frontend

Open this file directly in your browser:

```
client/index.html
```

âœ… Your chatbot is now live locally.

---

## ğŸŒ— Dark / Light Mode

* Toggle using the ğŸŒ™ / â˜€ï¸ icon
* Preference is stored in `localStorage`
* Automatically persists across reloads

---

## ğŸ“¦ Deployment Guide

### ğŸš€ Frontend Deployment (Vercel / Netlify)

You can deploy **only the frontend** easily.

#### Option 1: Vercel

1. Push code to GitHub
2. Go to [https://vercel.com](https://vercel.com)
3. Import the repository
4. Set **Root Directory** â†’ `client`
5. Deploy

#### Option 2: Netlify

1. Drag & drop the `client` folder
2. Done ğŸ‰

âš ï¸ Note: Backend must still run locally or on a server.

---

### ğŸš€ Backend Deployment (Optional)

To deploy backend:

* Use **Railway**, **Render**, or **EC2**
* Ensure Ollama is available on the server (or replace with cloud LLM)

---

## ğŸ” Environment Notes

* No API keys required
* AI runs **locally via Ollama**
* Safe for demos and portfolios

---

## ğŸ§ª Sample FAQs

```js
- Password reset
- Refund policy
- Contact support
- International shipping
```

(Defined in `server/faqs.js`)

---

## ğŸ› ï¸ Future Improvements

* Chat memory / conversation context
* Typing indicator animation
* Streaming responses
* User authentication
* Admin dashboard for FAQs
* React / Next.js frontend
* Docker support

---
